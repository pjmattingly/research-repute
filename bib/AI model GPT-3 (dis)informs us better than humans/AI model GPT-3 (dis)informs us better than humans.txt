AI model GPT-3 (dis)informs us better than humans
    see:
    https://www.science.org/doi/10.1126/sciadv.adh1850

    and:
    https://arxiv.org/ftp/arxiv/papers/2301/2301.11924.pdf

local:
https://github.com/pjmattingly/research-repute/blob/main/bib/AI%20model%20GPT-3%20(dis)informs%20us%20better%20than%20humans/2301.11924.pdf

----

"In this paper we evaluate whether
recruited individuals can distinguish disinformation from accurate information, structured in the form of
tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a
Twitter user or by the AI model GPT-3."

"Our results show that GPT-3 is a double-edge sword, which, in
comparison with humans, can produce accurate information that is easier to understand, but can also
produce more compelling disinformation."

"We also show that humans cannot distinguish tweets generated
by GPT-3 from tweets written by human users."

--

" In
fact, an initial test on people’s ability to tell whether a ∼ 500 word article was written by humans or GPT-3
showed a mean accuracy of 52%; just slightly better than chance 1
."

"...but also to produce
‘misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay
writing and social engineering pretexting’ 1,9–11"

"GPT-3 is an amplifier of human intentions which can
receive instructions in natural language and its output can be in natural or formal language."

the term the "fake news" and spam problem as an `infodemic`
    "The advancements in AI text generators and the release of GPT-3 coincide with the ongoing infodemic13 –
an epidemic-like circulation of fake news and disinformation, which, alongside the COVID-19 pandemic, has
been greatly detrimental for global health."

"In this paper we aim to determine whether GPT-3 can be used to produce accurate information and
disinformation in the form of tweets, and compare its credibility to that of human-generated information.
At the same time we aim to determine whether GPT-3 can be used to develop assistive tools to help
identifying disinformation. We acknowledge that the definitions of disinformation and misinformation are
diverse; in this paper we refer to an inclusive definition, which considers as disinformation both false
information (also partially false information) and/or misleading content14. "

"To achieve our goals, we asked GPT-3 to write information or disinformation tweets on topics prone to
disinformation and public misconception, such as vaccines, 5G, COVID-19, or the theory of evolution. We
collected real tweets on these topics and created a survey in which participants were asked to classify
synthetic tweets (written by GPT-3) and organic tweets (written by humans) as true or false and identify if
they were written by a real user or AI."

--

"To test GPT-3's ability to generate accurate or fake tweets, we created prompts instructing GPT-3 to
generate tweets with either accurate information or disinformation on the following topics: climate
change, vaccines safety, theory of evolution, COVID-19, masks safety, vaccines and autism, homeopathy
treatments for cancer, flat Earth, 5G technology and COVID-19, antibiotics and viral infections, and COVID19 and influenza. We performed a Twitter search to identify accurate tweets and disinformation tweets
generated by Twitter users."

--

"Participants
recognized “organic false” tweets with higher accuracy, compared with synthetic false tweets (Scores 0.92
versus 0.89, respectively; p=0.0032) (Figure 1C). Similarly, they recognized “synthetic true” tweets correctly
more than “organic true” tweets (Scores 0.84 versus 0.72, respectively; p<0.0001). This indicates that
human respondents can recognize the accuracy of tweets containing accurate information more often
when such tweets are generated by GPT-3, when compared with organic tweets retrieved from Twitter. "

"Similarly, GPT-3 generated disinformation tweets are more successful in misleading when compared with
organic ones, albeit with a small effect. When evaluating the same dataset, segmenting the analysis for true
versus false tweets – regardless of whether they are organic or synthetic – and for organic versus synthetic
tweets – regardless of their truthfulness – tweets containing accurate information received lower scores
when compared with tweets containing disinformation (Scores 0.78 versus 0.91, respectively; p<0.0001).
Similarly, synthetic tweets were categorized more often correctly for the accuracy of the information they
contained (Scores 0.87 versus 0.82, respectively; p<0.0001) (Figure 1C’). Participants required on average
29.14 seconds to determine whether an “organic true” tweet was accurate or contained disinformation.
This was significantly more when compared with “organic false” tweets, which required 23.28 seconds for
evaluation, with “synthetic true” tweets requiring 21.02 seconds, and “synthetic false” tweets requiring
19.87 seconds (Figure 1D). True tweets required a longer time for evaluation when compared with false
tweets (25.07 vs 21.97 seconds, p<0.0001), as well as organic tweets when compared with synthetic tweets
(26.21 vs 20.44 seconds, p<0.0001). (Figure 1D’)."

humans can't tell the difference between false and true tweet from GPT3
and they can parse such tweets more quickly, as compared with human written ones
    "This confirms that humans find it harder to assess
    accurate information than disinformation and GPT-3 is more efficient in informing and misleading humans,
    as respondents can evaluate its tweets faster, correctly or incorrectly."


Interestingly GPT3 had roughly comparable performance with humans when evaluating tweets as accurate or not
    "The respondents of our survey evaluated the accuracy or inaccuracy of the information contained in 220
    tweets. Using the same dataset, we asked GPT-3 to evaluate whether the tweets were accurate or whether
    they contained disinformation. For disinformation tweets, humans and GPT-3 performed similarly, although
    respondents performed slightly better (Success rates: 0.92 vs 0.89, respectively)."

    "For accurate tweets, GPT3, likewise human respondents, had more difficulties evaluating the accuracy of the information. In
    comparison, human respondents performed better than GPT-3 (Success rates: 0.72 vs 0.64, respectively)
    (Figure 2A). A detailed analysis of the results for each category of tweets can be found in Figure S6. These
    results suggest that human respondents can evaluate information better than GPT-3. Considering that
    these respondents are not necessarily trained individuals in recognizing disinformation, trained humans, at
    the time of writing, may perform much better than machines at performing this task."

--

"Information generated by GPT-3 is undistinguishable from human-generated information"

"We calculated the AI Recognition Score (or OS score), i.e., the ability of respondents to recognize whether
tweets are organic (i.e., produced by Twitter users) or synthetic (i.e., produced by GPT-3). As for the
evaluation of the ability to recognize disinformation, we calculated the AI Recognition Score for each type
of tweet (i.e., “organic true”, “synthetic true”, “organic false”, “synthetic false”). Respondents scored
around 0.5 on average, suggesting random choice between synthetic and organic tweets and inability to
distinguish AI-generated tweets from real ones (Figure 3)."

" Looking at true versus false tweets, and organic versus synthetic tweets, we
note something interesting; people's ability to determine organic versus synthetic tweets is not influenced
by tweet accuracy, with answers being random on average. (0.50 vs 0.50, respectively, p=0.9576). However,
when comparing organic versus synthetic tweets, organic tweets were recognized as organic more often
when compared with synthetic tweets recognized as synthetic (0.63 vs 0.37, p<0.0001) (Figure 3A’). "

"Therefore, both organic and synthetic tweets tend to be classified as “human”, indicating that GPT-3 can
effectively mimic human-generated information."

--

"At the beginning of the survey, we asked respondents to define how confident they were in their ability to
recognize disinformation, and in their ability to identify AI versus human-generated text, using a 1 to 5
Likert scale (Figure 4A,B). The Disinformation Recognition Confidence before the test was higher than AI
Recognition Confidence before the test (3.05 versus 2.69, respectively). After taking the survey (but before
seeing their results), we asked again respondents to define how confident they were in their ability to
recognize disinformation and AI versus human-generated text. Respondents were more confident in their
ability to recognize disinformation (Pre versus Post, 3.05 versus 3.49, respectively, p<0.0001) (Figure 4A),
whereas they were much less confident in their ability to recognize synthetic versus organic tweets (Pre
versus Post, 2.79 versus 1.70, respectively, p<0.0001) (Figure 4B). The increase in confidence to recognize
disinformation could be explained by the inoculation theory of misinformation17, whereby critical exposure
to disinformation could improve disinformation recognition capacity and resilience. "

interestingly they saw a marked lack of confidence from the surveyed people once they had gone through the experiment
they theorize that the people had "abandon their efforts to critically assess information" "when faced with a vast
amount of confusing information"
which is exactly what LLM's (e.g. ChatGPT) can produce in droves and for little cost
    " Instead, the stark
    decrease in confidence to recognize synthetic tweets could depend on what we could call “resignation 
    theory” – i.e., people may abandon their efforts to critically assess information when faced with a vast
    amount of confusing information. This could result in apathy and reliance on emotions for information
    consumption."

--

"Our findings show that tweets produced by GPT-3 can both inform and disinform better than organic
tweets"

"Synthetic tweets containing reliable information are recognized as true better and faster than true
organic tweets, while false synthetic tweets are recognized as false worse than false organic tweets."

And here we see that GPT seems to be able to produce high quality misinformation based on what it's trained on
that is, if GPT were trained on misinformation, it seems reasonable to assume that it could then produce more high quality disinformation
    ". Being GPT-3 a statistical representation of language – for how
    language is used in the datasets it was trained on – we assume GPT-3’s “disobedience” depends on the
    composition of GPT-3’s training datasets."

    "If the training dataset contains volumes of information
    contradicting what the prompt asks for, the system will likely output that type of information. We can
    therefore assume that the volume of information in the training dataset debunking causal links between
    vaccines and autism is higher than the volume of information debunking conspiracy theories on other
    topics taken into consideration by our study."

    "The risk of disinformation can be reduced by training it on regulated datasets that enhance accuracy and
    transparency, and report their sources. This would allow for independent fact checking."


--

here they discuss a simplistic approach to generate disinformation with GPT and ChatGPT more specifically
    "Based on these premises, synthetic text identification might soon be a hopeless battle to
    fight, for both people and AIs"

--

"Resignation theory"

"This decrease in self-confidence to distinguish synthetic text from organic text after exposure to both
synthetic and organic texts may be due to the realization that there is no clear marker that allows users to
identify whether a text has been generated by a machine or a human. "

"This is likely because of GPT-3's
ability to mimic human writing styles and language patterns."

"Additionally, respondents may have initially
underestimated GPT-3’s abilities to write human-like text: this may be due to the fact that such technology
is new and revolutionary, and people are not yet accustomed to how powerful it can be."

--

more as above, implying and stating that the training corpus is key to the creation of either factual or non-factual content
    "To mitigate negative effects, taking action to regulate which training datasets are used to
    develop these technologies is crucial, thus ensuring transparency, truthfulness of the output information,
    and limiting misuse of the technology to generate deceiving information. "

    "Until we do not have efficient
    strategies for identifying disinformation (whether based on human skills or on future AI improvements),
    restricting the use of these technologies may be necessary, e.g.: licensing them only to trusted users (e.g.,
    research institutions), or limiting the potential of AIs to certain type of applications. "

----

In sum

People can't tell the difference between human and AI generated tweets
after exposure to such an example of a combination of such information, they become much less confidence in their ability to tell the difference between human and AT generated Tweets
Then such AI models can produce disinformation better than humans (in speed and identification rate)
And while current AI models can have some difficulty intentially making disinforming tweets
training such models on false information may be able to increase their ability to make better disinforming tweets

This logically extends to AI crafting documents with false information that humans would have difficulty in recognizing as false.
And that a large volume of such disinformation, coupled with uncertainy as to what documents are real and which are generated by AI
may seriously impact the confidence of people attempting to determine which documents are real and those that are generated by AI.

This implies that flas documents generated by AI may be more easily accepted at face value, and alerting people that they may be consuming documents that are a mix of human and AI generated may hinder their ability further to identify such AI generated documents, and likely hindering their ability to logically evaluate such content.


TODO, add these citations to reading list
    1, 9–11